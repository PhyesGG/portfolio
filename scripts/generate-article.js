const fs = require('fs');
const path = require('path');

/**
 * Script de g√©n√©ration automatique d'article sur les serveurs RAG
 * Utilise Ollama en local pour g√©n√©rer du contenu
 */

const OLLAMA_API_URL = 'http://localhost:11434/api/generate';
const OLLAMA_MODEL = 'llama3.2:1b'; // Mod√®le l√©ger (1.3 GB) pour GitHub Actions

// Fallback : si Ollama √©choue, on g√©n√®re un article de base
function generateFallbackArticle() {
  const today = new Date();
  const lastWeek = new Date(today);
  lastWeek.setDate(lastWeek.getDate() - 7);

  const topics = [
    {
      title: "√âvolution des Serveurs RAG : Nouvelles Architectures",
      content: `# √âvolution des Serveurs RAG : Nouvelles Architectures\n\nLes serveurs RAG continuent d'√©voluer avec de nouvelles architectures plus performantes.\n\n## Tendances de la semaine\n\nCette semaine, plusieurs d√©veloppements majeurs ont marqu√© l'√©cosyst√®me RAG :\n\n### Optimisation des embeddings\nLes nouvelles techniques d'embedding permettent une recherche s√©mantique plus pr√©cise avec des bases de donn√©es vectorielles optimis√©es.\n\n### Mod√®les hybrides\nL'int√©gration de mod√®les locaux (Ollama) avec des APIs cloud offre le meilleur des deux mondes : confidentialit√© et performance.\n\n### Cas d'usage en entreprise\nDe plus en plus d'entreprises d√©ploient des serveurs RAG pour leurs bases de connaissances internes, am√©liorant significativement l'acc√®s √† l'information.`,
      tags: ["RAG", "IA", "Embeddings", "Entreprise"]
    },
    {
      title: "LangChain et Frameworks RAG : Nouveaut√©s",
      content: `# LangChain et Frameworks RAG : Nouveaut√©s\n\nLes frameworks pour serveurs RAG continuent de s'enrichir.\n\n## Mises √† jour importantes\n\n### LangChain\nNouvelles fonctionnalit√©s pour l'orchestration des cha√Ænes RAG, avec une meilleure gestion des contextes longs.\n\n### ChromaDB et Pinecone\nAm√©liorations des performances de recherche vectorielle, r√©duisant les temps de r√©ponse de 40%.\n\n### Int√©gration Ollama\nSupport am√©lior√© des mod√®les locaux pour une utilisation en production sans d√©pendance cloud.`,
      tags: ["LangChain", "ChromaDB", "Ollama", "RAG"]
    },
    {
      title: "Bases de Donn√©es Vectorielles : Performance et Scalabilit√©",
      content: `# Bases de Donn√©es Vectorielles : Performance et Scalabilit√©\n\nLes bases de donn√©es vectorielles sont au c≈ìur des serveurs RAG.\n\n## Innovations r√©centes\n\n### Weaviate et Qdrant\nNouvelles fonctionnalit√©s de clustering permettant de g√©rer des millions de vecteurs efficacement.\n\n### Optimisation des index\nAlgorithmes HNSW am√©lior√©s pour des recherches de similarit√© ultra-rapides.\n\n### D√©ploiement on-premise\nSolutions facilitant le d√©ploiement de bases vectorielles dans l'infrastructure existante des entreprises.`,
      tags: ["Vector DB", "Weaviate", "Performance", "RAG"]
    }
  ];

  const randomTopic = topics[Math.floor(Math.random() * topics.length)];

  return {
    id: `article-${today.toISOString().split('T')[0]}`,
    title: randomTopic.title,
    summary: `Article de veille hebdomadaire sur les serveurs RAG - P√©riode du ${lastWeek.toLocaleDateString('fr-FR')} au ${today.toLocaleDateString('fr-FR')}`,
    content: randomTopic.content,
    date: today.toISOString(),
    tags: randomTopic.tags,
    sources: [
      "https://www.langchain.com/",
      "https://ollama.ai/",
      "https://www.pinecone.io/"
    ]
  };
}

async function generateWithOllama(prompt) {
  try {
    const response = await fetch(OLLAMA_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: OLLAMA_MODEL,
        prompt: prompt,
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.status}`);
    }

    const data = await response.json();
    return data.response;
  } catch (error) {
    console.error('Erreur lors de la g√©n√©ration avec Ollama:', error);
    throw error;
  }
}

async function generateArticle() {
  const today = new Date();
  const lastWeek = new Date(today);
  lastWeek.setDate(lastWeek.getDate() - 7);

  const prompt = `Tu es un expert en intelligence artificielle et en serveurs RAG (Retrieval-Augmented Generation).

R√©dige un article de veille technologique hebdomadaire sur les serveurs RAG et l'IA en entreprise.

L'article doit :
- √ätre r√©dig√© en fran√ßais
- Faire environ 500-700 mots
- Couvrir les nouveaut√©s et tendances de la semaine derni√®re (du ${lastWeek.toLocaleDateString('fr-FR')} au ${today.toLocaleDateString('fr-FR')})
- Inclure des informations sur les nouvelles technologies, frameworks, ou cas d'usage des serveurs RAG
- √ätre structur√© avec des titres et sous-titres (utilise # pour les titres en markdown)
- √ätre professionnel mais accessible
- Mentionner des technologies concr√®tes (LangChain, Ollama, ChromaDB, Pinecone, etc.)

Format de r√©ponse souhait√© en JSON :
{
  "title": "Titre accrocheur de l'article",
  "summary": "R√©sum√© en 2-3 phrases",
  "content": "Contenu complet de l'article en markdown. Utilise \\n pour les sauts de ligne. Echappe les guillemets avec \\\".",
  "tags": ["tag1", "tag2", "tag3", "tag4"],
  "sources": ["https://example.com/source1", "https://example.com/source2"]
}

CRITIQUES IMPORTANTES :
- R√©ponds UNIQUEMENT avec le JSON valide, aucun texte avant ou apr√®s
- Utilise \\n pour les sauts de ligne dans le contenu
- Echappe tous les guillemets dans le contenu avec \\\"
- Le contenu doit √™tre une seule cha√Æne de caract√®res
- Assure-toi que le JSON est parfaitement format√© et valide`;

  console.log('ü§ñ G√©n√©ration de l\'article avec Ollama...');
  const response = await generateWithOllama(prompt);

  console.log('üìÑ R√©ponse brute d\'Ollama (premiers 500 caract√®res):');
  console.log(response.substring(0, 500));

  // Essayer d'extraire le JSON de la r√©ponse
  let articleData;
  try {
    // Nettoyer la r√©ponse avant de parser
    let cleanedResponse = response.trim();

    // Extraire le JSON s'il y a du texte avant/apr√®s
    const jsonMatch = cleanedResponse.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      cleanedResponse = jsonMatch[0];
    }

    // Tenter de parser
    articleData = JSON.parse(cleanedResponse);
  } catch (e) {
    console.error('‚ùå Erreur de parsing JSON:', e.message);
    console.log('üìù Tentative de nettoyage avanc√©...');

    try {
      // Nettoyage plus agressif
      let cleanedResponse = response
        .replace(/[\x00-\x1F\x7F-\x9F]/g, '') // Supprimer les caract√®res de contr√¥le
        .trim();

      const jsonMatch = cleanedResponse.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        articleData = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('Impossible d\'extraire un JSON valide de la r√©ponse');
      }
    } catch (e2) {
      console.error('‚ùå √âchec du nettoyage avanc√©');
      throw new Error(`Erreur de parsing JSON: ${e.message}. R√©ponse: ${response.substring(0, 200)}`);
    }
  }

  // Cr√©er l'article avec un ID unique
  const article = {
    id: `article-${today.toISOString().split('T')[0]}`,
    title: articleData.title,
    summary: articleData.summary,
    content: articleData.content,
    date: today.toISOString(),
    tags: articleData.tags || ['RAG', 'IA', 'Veille'],
    sources: articleData.sources || [],
  };

  return article;
}

async function saveArticle(article) {
  const articlesPath = path.join(__dirname, '..', 'data', 'articles.json');

  // Lire le fichier existant
  const data = JSON.parse(fs.readFileSync(articlesPath, 'utf8'));

  // V√©rifier si un article existe d√©j√† pour aujourd'hui
  const existingIndex = data.articles.findIndex(a => a.id === article.id);

  if (existingIndex !== -1) {
    console.log('‚ö†Ô∏è  Un article existe d√©j√† pour aujourd\'hui, mise √† jour...');
    data.articles[existingIndex] = article;
  } else {
    console.log('‚úÖ Ajout du nouvel article...');
    data.articles.unshift(article); // Ajouter au d√©but
  }

  // Sauvegarder
  fs.writeFileSync(articlesPath, JSON.stringify(data, null, 2), 'utf8');
  console.log('üíæ Article sauvegard√© avec succ√®s !');
}

async function main() {
  try {
    console.log('üöÄ D√©marrage de la g√©n√©ration d\'article...\n');

    let article;

    try {
      // Essayer de g√©n√©rer avec Ollama
      article = await generateArticle();
    } catch (ollamaError) {
      console.warn('‚ö†Ô∏è  Ollama a √©chou√©, utilisation du mode fallback...');
      console.warn('Erreur Ollama:', ollamaError.message);

      // Utiliser le fallback
      article = generateFallbackArticle();
      console.log('‚úÖ Article fallback g√©n√©r√©');
    }

    console.log('\nüìù Article g√©n√©r√© :');
    console.log('Titre:', article.title);
    console.log('R√©sum√©:', article.summary);
    console.log('Tags:', article.tags.join(', '));
    console.log('\n');

    await saveArticle(article);

    console.log('\nüéâ Processus termin√© avec succ√®s !');
    process.exit(0);
  } catch (error) {
    console.error('\n‚ùå Erreur critique lors de la g√©n√©ration de l\'article:', error);
    process.exit(1);
  }
}

main();
